{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"student.xls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file):\n",
    "    lvl_map = {\n",
    "        \"High\" : 0,\n",
    "        \"Middle\" : 1,\n",
    "        \"Low\" : 2,\n",
    "        \"very_low\" : 3,\n",
    "        \"Very Low\" : 3\n",
    "    }\n",
    "    train_ori = pd.read_excel(file, sheet_name=\"Training_Data\")\n",
    "    test_ori = pd.read_excel(file, sheet_name=\"Test_Data\")\n",
    "    train_label = np.array(train_ori.iloc[:, -1].map(lvl_map)).astype(np.int).reshape((-1, ))\n",
    "    test_label = np.array(test_ori.iloc[:, -1].map(lvl_map)).astype(np.int).reshape((-1, ))\n",
    "    train_data = np.delete(np.array(train_ori), -1, -1).astype(np.float)\n",
    "    test_data = np.delete(np.array(test_ori), -1, -1).astype(np.float)\n",
    "    return train_data, train_label, test_data, test_label, lvl_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label, test_data, test_label, lvl_map = load_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    def __init__(self, W, b, alpha_0, decay, epoch_drop):\n",
    "        self.W = W.copy()\n",
    "        self.b = b.copy()\n",
    "        self.alpha_0 = alpha_0\n",
    "        self.decay = decay\n",
    "        self.epoch_drop = epoch_drop\n",
    "        self.count = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x.copy()\n",
    "        self.m, self.n = x.shape\n",
    "        return np.dot(self.x, self.W) + self.b\n",
    "\n",
    "    def backprop(self, back_grad):\n",
    "        self.grad_W = np.dot(self.x.T, back_grad)\n",
    "        self.grad_b = np.dot(np.ones(self.m), back_grad)\n",
    "        self.grad = np.dot(back_grad, self.W.T)\n",
    "        return self.grad\n",
    "\n",
    "    def l_rate(self):\n",
    "        lrate = self.alpha_0 * (self.decay ** (np.floor((1 + self.count) / self.epoch_drop)))\n",
    "        self.count += 1\n",
    "        return lrate\n",
    "    \n",
    "    def update(self):\n",
    "        alpha = self.l_rate()\n",
    "        self.W -= alpha * self.grad_W\n",
    "        self.b -= alpha * self.grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self, x):\n",
    "        self.x = x.copy()\n",
    "        self.sig_res = 1 / (1 + np.exp(-x))\n",
    "        return self.sig_res\n",
    "    \n",
    "    def backprop(self, back_grad):\n",
    "        grad = back_grad * self.sig_res * (1 - self.sig_res)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def forward(self, x):\n",
    "        self.x = x.copy()\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def backprop(self, back_grad):\n",
    "        grad = back_grad.copy()\n",
    "        grad[self.x < 0] = 0\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaky_Relu:\n",
    "    def forward(self, x):\n",
    "        self.x = x.copy()\n",
    "        return np.maximum(x, self.x * 0.01)\n",
    "\n",
    "    def backprop(self, back_grad):\n",
    "        grad = back_grad.copy()\n",
    "        grad[self.x < 0] = grad[self.x < 0] * 0.01\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        self.x = x.copy()\n",
    "        self.tanh = np.tanh(x)\n",
    "        return self.tanh\n",
    "    \n",
    "    def backprop(self, back_grad):\n",
    "        grad = back_grad * (1 - self.tanh ** 2)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arctan:\n",
    "    def forward(self, x):\n",
    "        self.x = x.copy()\n",
    "        return np.arctan(self.x)\n",
    "    \n",
    "    def backprop(self, back_grad):\n",
    "        grad = back_grad / (1 + self.x ** 2)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftPlus:\n",
    "    def forward(self, x):\n",
    "        self.x = x.copy()\n",
    "        return np.log(1 + np.exp(self.x))\n",
    "    \n",
    "    def backprop(self, back_grad):\n",
    "        grad = back_grad / (1 + np.exp(-self.x))\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftSign:\n",
    "    def forward(self, x):\n",
    "        self.x = x.copy()\n",
    "        return self.x / (1 + np.abs(self.x))\n",
    "    \n",
    "    def backprop(self, back_grad):\n",
    "        grad = back_grad / (1 + np.abs(self.x) ** 2)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, x, y):\n",
    "        self.x = (x.copy() - x.max(axis=1).reshape(-1, 1))\n",
    "        self.y = y.copy()\n",
    "        self.m, self.n = self.x.shape\n",
    "        self.denom = np.sum(np.exp(x), axis=1).reshape((-1, 1))\n",
    "        self.softmax = np.exp(x) / self.denom\n",
    "        loss = 0\n",
    "        for i in range(self.m):\n",
    "            loss -= np.log(self.softmax[i, y[i]])\n",
    "        return loss / self.m\n",
    "\n",
    "    def backprop(self):\n",
    "        Dirac = lambda x, y : 1 if x == y else 0\n",
    "        grad = np.zeros([self.m, self.n])\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.n):\n",
    "                grad[i, j] = (self.softmax[i, j] - Dirac(j ,self.y[i])) / self.m\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp(train_data, train_label, test_data, test_label, epochs, activation, alpha_0, decay, epochs_drop):\n",
    "    W1 = np.random.randn(5, 16) / np.sqrt(6)\n",
    "    b1 = np.zeros(16)\n",
    "    W2 = np.random.randn(16, 4) / np.sqrt(6)\n",
    "    b2 = np.zeros(4)\n",
    "    \n",
    "    activation_function_dict = {\n",
    "        \"arctan\"   : Arctan,\n",
    "        \"l_relu\"   : Leaky_Relu, \n",
    "        \"relu\"     : Relu, \n",
    "        \"sigmoid\"  : Sigmoid, \n",
    "        \"tanh\"     : Tanh, \n",
    "        \"softplus\" : SoftPlus,\n",
    "        \"softsign\" : SoftSign\n",
    "    }\n",
    "    \n",
    "    fc1 = FC(W1, b1, alpha_0, decay, epochs_drop)\n",
    "    act_f1 = activation_function_dict[activation]()\n",
    "    fc2 = FC(W2, b2, alpha_0, decay, epochs_drop)\n",
    "    softmax = Softmax()\n",
    "\n",
    "    for i in range(1, epochs + 1):\n",
    "        h1 = fc1.forward(train_data)\n",
    "        h2 = act_f1.forward(h1)\n",
    "        h3 = fc2.forward(h2)\n",
    "        loss = softmax.forward(h3, train_label)\n",
    "\n",
    "        if i % (epochs / 5) == 0:\n",
    "            print(\"After %d/%d epochs, loss : %f\" % (i, epochs, loss))\n",
    "\n",
    "        h3_grad = softmax.backprop()\n",
    "        h2_grad = fc2.backprop(h3_grad)\n",
    "        fc2.update()\n",
    "        h1_grad = act_f1.backprop(h2_grad)\n",
    "        x_grad = fc1.backprop(h1_grad)  # x_grad is useless in this supervised learning\n",
    "        fc1.update()\n",
    "        \n",
    "    test_h1 = fc1.forward(test_data)\n",
    "    test_h2 = act_f1.forward(test_h1)\n",
    "    test_h3 = fc2.forward(test_h2)\n",
    "    pred = np.argmax(test_h3, 1)\n",
    "\n",
    "    acc = np.mean(pred == test_label)\n",
    "\n",
    "    print('test acc: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method : sigmoid\n",
      "After 1000/5000 epochs, loss : 0.818032\n",
      "After 2000/5000 epochs, loss : 0.517375\n",
      "After 3000/5000 epochs, loss : 0.368986\n",
      "After 4000/5000 epochs, loss : 0.288218\n",
      "After 5000/5000 epochs, loss : 0.239265\n",
      "test acc:  0.9586206896551724\n",
      "------------------------------------\n",
      "Method : tanh\n",
      "After 1000/5000 epochs, loss : 0.261168\n",
      "After 2000/5000 epochs, loss : 0.158584\n",
      "After 3000/5000 epochs, loss : 0.130279\n",
      "After 4000/5000 epochs, loss : 0.117600\n",
      "After 5000/5000 epochs, loss : 0.110116\n",
      "test acc:  0.9724137931034482\n",
      "------------------------------------\n",
      "Method : arctan\n",
      "After 1000/5000 epochs, loss : 0.263093\n",
      "After 2000/5000 epochs, loss : 0.159760\n",
      "After 3000/5000 epochs, loss : 0.130428\n",
      "After 4000/5000 epochs, loss : 0.117183\n",
      "After 5000/5000 epochs, loss : 0.109511\n",
      "test acc:  0.9724137931034482\n",
      "------------------------------------\n",
      "Method : relu\n",
      "After 1000/5000 epochs, loss : 0.246043\n",
      "After 2000/5000 epochs, loss : 0.146018\n",
      "After 3000/5000 epochs, loss : 0.121045\n",
      "After 4000/5000 epochs, loss : 0.109989\n",
      "After 5000/5000 epochs, loss : 0.103550\n",
      "test acc:  0.9724137931034482\n",
      "------------------------------------\n",
      "Method : l_relu\n",
      "After 1000/5000 epochs, loss : 0.239020\n",
      "After 2000/5000 epochs, loss : 0.145966\n",
      "After 3000/5000 epochs, loss : 0.121500\n",
      "After 4000/5000 epochs, loss : 0.110560\n",
      "After 5000/5000 epochs, loss : 0.103965\n",
      "test acc:  0.9793103448275862\n",
      "------------------------------------\n",
      "Method : softplus\n",
      "After 1000/5000 epochs, loss : 0.283647\n",
      "After 2000/5000 epochs, loss : 0.157458\n",
      "After 3000/5000 epochs, loss : 0.128404\n",
      "After 4000/5000 epochs, loss : 0.116581\n",
      "After 5000/5000 epochs, loss : 0.110187\n",
      "test acc:  0.9793103448275862\n",
      "------------------------------------\n",
      "Method : softsign\n",
      "After 1000/5000 epochs, loss : 0.317389\n",
      "After 2000/5000 epochs, loss : 0.178670\n",
      "After 3000/5000 epochs, loss : 0.137825\n",
      "After 4000/5000 epochs, loss : 0.120143\n",
      "After 5000/5000 epochs, loss : 0.110392\n",
      "test acc:  0.9724137931034482\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "activation_function_list = [\n",
    "    \"sigmoid\", \n",
    "    \"tanh\", \n",
    "    \"arctan\", \n",
    "    \"relu\", \n",
    "    \"l_relu\", \n",
    "    \"softplus\",\n",
    "    \"softsign\"\n",
    "]\n",
    "\n",
    "for method in activation_function_list:\n",
    "    print(\"Method : %s\" % method)\n",
    "    bp(train_data, \n",
    "       train_label, \n",
    "       test_data, \n",
    "       test_label, \n",
    "       epochs=5000,\n",
    "       activation=method, \n",
    "       alpha_0=0.1, \n",
    "       decay=0.99, \n",
    "       epochs_drop=1000)\n",
    "    print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
