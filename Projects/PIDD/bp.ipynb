{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"PIDD.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    train_data_ori = pd.read_excel(path, sheet_name='traindata', header=None)\n",
    "    train_label = np.array(train_data_ori.iloc[:, -1])\n",
    "    train_data = np.array(train_data_ori)[:, :-1]\n",
    "    test_data_ori = pd.read_excel(path, sheet_name='testdata', header=None)\n",
    "    test_label = np.array(test_data_ori.iloc[:, -1])\n",
    "    test_data = np.array(test_data_ori)[:, :-1]\n",
    "    \n",
    "#     train_mean = train_data.mean(axis=0)\n",
    "#     test_mean = test_data.mean(axis=0)\n",
    "#     train_var = train_data.var(axis=0)\n",
    "#     test_var = test_data.var(axis=0)\n",
    "#     train_data = (train_data - train_mean) / train_var\n",
    "#     test_data = (test_data - test_mean) / test_var\n",
    "    \n",
    "    train_min = train_data.min(axis=0)\n",
    "    test_min = test_data.min(axis=0)\n",
    "    train_max = train_data.max(axis=0)\n",
    "    test_max = test_data.max(axis=0)\n",
    "    train_data = (train_data + train_min) / (train_max - train_min)\n",
    "    test_data = (test_data + test_min) / (test_max - test_min)\n",
    "    \n",
    "    return train_data, train_label, test_data, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label, test_data, test_label = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    def __init__(self, W, b, alpha_0, decay, epoch_drop):\n",
    "        self.W = W.copy()\n",
    "        self.b = b.copy()\n",
    "        self.alpha_0 = alpha_0\n",
    "        self.decay = decay\n",
    "        self.epoch_drop = epoch_drop\n",
    "        self.count = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x.copy()\n",
    "        self.m, self.n = x.shape\n",
    "        return np.dot(self.x, self.W) + self.b\n",
    "\n",
    "    def backprop(self, back_grad):\n",
    "        self.grad_W = np.dot(self.x.T, back_grad)\n",
    "        self.grad_b = np.dot(np.ones(self.m), back_grad)\n",
    "        self.grad = np.dot(back_grad, self.W.T)\n",
    "        return self.grad\n",
    "\n",
    "    def l_rate(self):\n",
    "        lrate = self.alpha_0 * (self.decay ** (np.floor((1 + self.count) / self.epoch_drop)))\n",
    "        self.count += 1\n",
    "        return lrate\n",
    "    \n",
    "    def update(self):\n",
    "        alpha = self.l_rate()\n",
    "        self.W -= alpha * self.grad_W\n",
    "        self.b -= alpha * self.grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self, x):\n",
    "        self.x = x.copy()\n",
    "        self.sig_res = 1 / (1 + np.exp(-x))\n",
    "        return self.sig_res\n",
    "    \n",
    "    def backprop(self, back_grad):\n",
    "        grad = back_grad * self.sig_res * (1 - self.sig_res)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def forward(self, x):\n",
    "        self.x = x.copy()\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def backprop(self, back_grad):\n",
    "        grad = back_grad.copy()\n",
    "        grad[self.x < 0] = 0\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaky_Relu:\n",
    "    def forward(self, x):\n",
    "        self.x = x.copy()\n",
    "        return np.maximum(x, self.x * 0.01)\n",
    "\n",
    "    def backprop(self, back_grad):\n",
    "        grad = back_grad.copy()\n",
    "        grad[self.x < 0] = grad[self.x < 0] * 0.01\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        self.x = x.copy()\n",
    "        self.tanh = np.tanh(x)\n",
    "        return self.tanh\n",
    "    \n",
    "    def backprop(self, back_grad):\n",
    "        grad = back_grad * (1 - self.tanh ** 2)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arctan:\n",
    "    def forward(self, x):\n",
    "        self.x = x.copy()\n",
    "        return np.arctan(self.x)\n",
    "    \n",
    "    def backprop(self, back_grad):\n",
    "        grad = back_grad / (1 + self.x ** 2)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftPlus:\n",
    "    def forward(self, x):\n",
    "        self.x = x.copy()\n",
    "        return np.log(1 + np.exp(self.x))\n",
    "    \n",
    "    def backprop(self, back_grad):\n",
    "        grad = back_grad / (1 + np.exp(-self.x))\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftSign:\n",
    "    def forward(self, x):\n",
    "        self.x = x.copy()\n",
    "        return self.x / (1 + np.abs(self.x))\n",
    "    \n",
    "    def backprop(self, back_grad):\n",
    "        grad = back_grad / (1 + np.abs(self.x) ** 2)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, x, y):\n",
    "        self.x = (x.copy() - x.max(axis=1).reshape(-1, 1))\n",
    "        self.y = y.copy()\n",
    "        self.m, self.n = self.x.shape\n",
    "        self.denom = np.sum(np.exp(x), axis=1).reshape((-1, 1))\n",
    "        self.softmax = np.exp(x) / self.denom\n",
    "        loss = 0\n",
    "        for i in range(self.m):\n",
    "            loss -= np.log(self.softmax[i, y[i]])\n",
    "        return loss / self.m\n",
    "\n",
    "    def dirac(self, a, b):\n",
    "        return 1 if a == b else 0\n",
    "    \n",
    "    def backprop(self):\n",
    "        grad = np.zeros([self.m, self.n])\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.n):\n",
    "                grad[i, j] = (self.softmax[i, j] - self.dirac(j, self.y[i])) / self.m\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp(train_data, \n",
    "       train_label, \n",
    "       test_data, \n",
    "       test_label, \n",
    "       epochs, \n",
    "       activation, \n",
    "       alpha_0, \n",
    "       decay, \n",
    "       epochs_drop, \n",
    "       result_require=False):\n",
    "    \n",
    "    W1 = np.random.randn(8, 17) / np.sqrt(6)\n",
    "    b1 = np.zeros(17)\n",
    "    W2 = np.random.randn(17, 7) / np.sqrt(6)\n",
    "    b2 = np.zeros(7)\n",
    "    W3 = np.random.randn(7, 2) / np.sqrt(6)\n",
    "    b3 = np.random.randn(2)\n",
    "    \n",
    "    activation_function_dict = {\n",
    "        \"arctan\"   : Arctan,\n",
    "        \"l_relu\"   : Leaky_Relu, \n",
    "        \"relu\"     : Relu, \n",
    "        \"sigmoid\"  : Sigmoid, \n",
    "        \"tanh\"     : Tanh, \n",
    "        \"softplus\" : SoftPlus,\n",
    "        \"softsign\" : SoftSign\n",
    "    }\n",
    "    \n",
    "    fc1 = FC(W1, b1, alpha_0, decay, epochs_drop)\n",
    "    act_f1 = activation_function_dict[activation]()\n",
    "    fc2 = FC(W2, b2, alpha_0, decay, epochs_drop)\n",
    "    act_f2 = activation_function_dict[activation]()\n",
    "    fc3 = FC(W3, b3, alpha_0, decay, epochs_drop)\n",
    "    softmax = Softmax()\n",
    "\n",
    "    # TRAINING BEGIN\n",
    "    for i in range(1, epochs + 1):\n",
    "        h1 = fc1.forward(train_data)\n",
    "        h2 = act_f1.forward(h1)\n",
    "        h3 = fc2.forward(h2)\n",
    "        h4 = act_f2.forward(h3)\n",
    "        h5 = fc3.forward(h4)\n",
    "        loss = softmax.forward(h5, train_label)\n",
    "\n",
    "        if i % (epochs / 5) == 0:\n",
    "            print(\"After %d/%d epochs, loss : %f\" % (i, epochs, loss))\n",
    "\n",
    "        h5_grad = softmax.backprop()\n",
    "        h4_grad = fc3.backprop(h5_grad)\n",
    "        fc3.update()\n",
    "        h3_grad = act_f2.backprop(h4_grad)\n",
    "        h2_grad = fc2.backprop(h3_grad)\n",
    "        fc2.update()\n",
    "        h1_grad = act_f1.backprop(h2_grad)\n",
    "        x_grad = fc1.backprop(h1_grad)  # x_grad is useless in this supervised learning\n",
    "        fc1.update()\n",
    "    # TRAINING FINISH\n",
    "    \n",
    "    train_h1 = fc1.forward(train_data)\n",
    "    train_h2 = act_f1.forward(train_h1)\n",
    "    train_h3 = fc2.forward(train_h2)\n",
    "    train_h4 = act_f2.forward(train_h3)\n",
    "    train_h5 = fc3.forward(train_h4)\n",
    "    train_pred = np.argmax(train_h5, 1)\n",
    "    train_acc = np.mean(train_pred == train_label)\n",
    "    print('train acc: ', train_acc)\n",
    "    \n",
    "    test_h1 = fc1.forward(test_data)\n",
    "    test_h2 = act_f1.forward(test_h1)\n",
    "    test_h3 = fc2.forward(test_h2)\n",
    "    test_h4 = act_f2.forward(test_h3)\n",
    "    test_h5 = fc3.forward(test_h4)\n",
    "    test_pred = np.argmax(test_h5, 1)\n",
    "    test_acc = np.mean(test_pred == test_label)\n",
    "    print('test acc: ', test_acc)\n",
    "    \n",
    "    if result_require == True:\n",
    "        return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method : sigmoid\n",
      "After 200/1000 epochs, loss : 0.643664\n",
      "After 400/1000 epochs, loss : 0.643231\n",
      "After 600/1000 epochs, loss : 0.642791\n",
      "After 800/1000 epochs, loss : 0.642332\n",
      "After 1000/1000 epochs, loss : 0.641844\n",
      "train acc:  0.65625\n",
      "test acc:  0.625\n",
      "------------------------------------\n",
      "Method : tanh\n",
      "After 200/1000 epochs, loss : 0.580397\n",
      "After 400/1000 epochs, loss : 0.503426\n",
      "After 600/1000 epochs, loss : 0.468613\n",
      "After 800/1000 epochs, loss : 0.457285\n",
      "After 1000/1000 epochs, loss : 0.453795\n",
      "train acc:  0.784375\n",
      "test acc:  0.6875\n",
      "------------------------------------\n",
      "Method : arctan\n",
      "After 200/1000 epochs, loss : 0.579688\n",
      "After 400/1000 epochs, loss : 0.493039\n",
      "After 600/1000 epochs, loss : 0.461288\n",
      "After 800/1000 epochs, loss : 0.455138\n",
      "After 1000/1000 epochs, loss : 0.453355\n",
      "train acc:  0.78125\n",
      "test acc:  0.6640625\n",
      "------------------------------------\n",
      "Method : relu\n",
      "After 200/1000 epochs, loss : 0.613596\n",
      "After 400/1000 epochs, loss : 0.550365\n",
      "After 600/1000 epochs, loss : 0.507724\n",
      "After 800/1000 epochs, loss : 0.484639\n",
      "After 1000/1000 epochs, loss : 0.471784\n",
      "train acc:  0.7765625\n",
      "test acc:  0.6796875\n",
      "------------------------------------\n",
      "Method : l_relu\n",
      "After 200/1000 epochs, loss : 0.559163\n",
      "After 400/1000 epochs, loss : 0.493898\n",
      "After 600/1000 epochs, loss : 0.466551\n",
      "After 800/1000 epochs, loss : 0.458829\n",
      "After 1000/1000 epochs, loss : 0.455344\n",
      "train acc:  0.7828125\n",
      "test acc:  0.703125\n",
      "------------------------------------\n",
      "Method : softplus\n",
      "After 200/1000 epochs, loss : 0.592478\n",
      "After 400/1000 epochs, loss : 0.528263\n",
      "After 600/1000 epochs, loss : 0.483511\n",
      "After 800/1000 epochs, loss : 0.471059\n",
      "After 1000/1000 epochs, loss : 0.467035\n",
      "train acc:  0.7859375\n",
      "test acc:  0.71875\n",
      "------------------------------------\n",
      "Method : softsign\n",
      "After 200/1000 epochs, loss : 0.642054\n",
      "After 400/1000 epochs, loss : 0.626872\n",
      "After 600/1000 epochs, loss : 0.590314\n",
      "After 800/1000 epochs, loss : 0.519541\n",
      "After 1000/1000 epochs, loss : 0.473011\n",
      "train acc:  0.78125\n",
      "test acc:  0.6953125\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "activation_function_list = [\n",
    "    \"sigmoid\", \n",
    "    \"tanh\", \n",
    "    \"arctan\", \n",
    "    \"relu\", \n",
    "    \"l_relu\", \n",
    "    \"softplus\",\n",
    "    \"softsign\"\n",
    "]\n",
    "\n",
    "for method in activation_function_list:\n",
    "    print(\"Method : %s\" % method)\n",
    "    bp(train_data, \n",
    "       train_label, \n",
    "       test_data, \n",
    "       test_label, \n",
    "       epochs=1000,\n",
    "       activation=method, \n",
    "       alpha_0=0.05, \n",
    "       decay=0.99, \n",
    "       epochs_drop=100)\n",
    "    print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 200/1000 epochs, loss : 0.491370\n",
      "After 400/1000 epochs, loss : 0.473943\n",
      "After 600/1000 epochs, loss : 0.462637\n",
      "After 800/1000 epochs, loss : 0.457157\n",
      "After 1000/1000 epochs, loss : 0.456410\n",
      "train acc:  0.7828125\n",
      "test acc:  0.75\n"
     ]
    }
   ],
   "source": [
    "res = bp(train_data, \n",
    "         train_label, \n",
    "         test_data, \n",
    "         test_label, \n",
    "         epochs=1000,\n",
    "         activation=\"l_relu\", \n",
    "         alpha_0=0.1, \n",
    "         decay=0.99, \n",
    "         epochs_drop=1000, \n",
    "         result_require=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
